{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Whisper Fine-tuning for Low-Resource African Languages\n",
    "\n",
    "This notebook implements **Phase 3** of the Low-Resource Speech-to-Speech pipeline, focusing on fine-tuning OpenAI Whisper for African languages.\n",
    "\n",
    "## Why Whisper for Low-Resource Languages?\n",
    "\n",
    "Based on our research, Whisper is ideal because:\n",
    "- ‚úÖ **Robust to noise and accents** - Critical for real-world African audio\n",
    "- ‚úÖ **Excellent zero-shot performance** - Good baseline even without fine-tuning\n",
    "- ‚úÖ **Multiple model sizes** - From tiny (39M) to large-v3 (1.5B) parameters\n",
    "- ‚úÖ **Fine-tunable with limited data** - As little as a few hours of quality audio\n",
    "- ‚úÖ **Straightforward fine-tuning** - Well-supported by Hugging Face\n",
    "\n",
    "## Supported African Languages\n",
    "\n",
    "Whisper has some built-in support for:\n",
    "- **Swahili** (sw) - Good baseline\n",
    "- **Hausa** (ha) - Limited support\n",
    "- **Yoruba** (yo) - Limited support  \n",
    "- **Arabic** (ar) - Excellent support\n",
    "- **Somali** (so) - Limited support\n",
    "\n",
    "This notebook will help improve performance through fine-tuning with your specific data.\n",
    "\n",
    "## Pipeline Overview\n",
    "\n",
    "1. **Data Preparation** - Process your audio + transcription data\n",
    "2. **Synthetic Data Integration** - Combine with generated data from Phase 2\n",
    "3. **Model Fine-tuning** - Adapt Whisper to your target language\n",
    "4. **Evaluation** - Measure WER (Word Error Rate) improvements\n",
    "5. **Deployment** - Export for inference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Phase 1: Environment Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages for Whisper fine-tuning\n",
    "!pip install -q torch torchaudio transformers datasets accelerate evaluate jiwer\n",
    "!pip install -q librosa soundfile pydub\n",
    "!pip install -q huggingface_hub wandb tensorboard\n",
    "\n",
    "print(\"‚úÖ Whisper fine-tuning environment ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Union, Any, Optional\n",
    "\n",
    "# Transformers and training\n",
    "from transformers import (\n",
    "    WhisperFeatureExtractor,\n",
    "    WhisperTokenizer, \n",
    "    WhisperProcessor,\n",
    "    WhisperForConditionalGeneration,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    TrainerCallback\n",
    ")\n",
    "\n",
    "# Dataset and evaluation\n",
    "from datasets import Dataset, DatasetDict, load_dataset, Audio\n",
    "from evaluate import load\n",
    "import jiwer\n",
    "\n",
    "# Audio processing\n",
    "from pydub import AudioSegment\n",
    "from pydub.utils import which\n",
    "\n",
    "# Utilities\n",
    "import json\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"‚úÖ PyTorch version: {torch.__version__}\")\n",
    "print(f\"‚úÖ CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Configuration for African Language Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for low-resource African language fine-tuning\n",
    "CONFIG = {\n",
    "    # Model settings\n",
    "    \"model_name_or_path\": \"openai/whisper-small\",  # Options: tiny, base, small, medium, large-v3\n",
    "    \"language\": \"sw\",  # Target language code (sw=Swahili, ha=Hausa, yo=Yoruba, etc.)\n",
    "    \"language_name\": \"Swahili\",  # Human readable language name\n",
    "    \"task\": \"transcribe\",  # \"transcribe\" or \"translate\"\n",
    "    \n",
    "    # Data settings\n",
    "    \"dataset_name\": \"your-username/african-speech-dataset\",  # Your Hugging Face dataset\n",
    "    \"audio_column\": \"audio\",\n",
    "    \"transcript_column\": \"sentence\",\n",
    "    \"test_size\": 0.2,  # Fraction for test set\n",
    "    \n",
    "    # Audio processing\n",
    "    \"sampling_rate\": 16000,  # Whisper expects 16kHz\n",
    "    \"max_audio_length\": 30.0,  # Maximum audio length in seconds\n",
    "    \"min_audio_length\": 1.0,   # Minimum audio length in seconds\n",
    "    \n",
    "    # Training settings\n",
    "    \"output_dir\": \"./whisper-small-swahili\",  # Change based on your language\n",
    "    \"per_device_train_batch_size\": 8,  # Adjust based on GPU memory\n",
    "    \"per_device_eval_batch_size\": 8,\n",
    "    \"gradient_accumulation_steps\": 2,\n",
    "    \"learning_rate\": 1e-5,\n",
    "    \"warmup_steps\": 500,\n",
    "    \"max_steps\": 5000,  # Adjust based on dataset size\n",
    "    \"gradient_checkpointing\": True,\n",
    "    \"fp16\": True,\n",
    "    \"evaluation_strategy\": \"steps\",\n",
    "    \"eval_steps\": 500,\n",
    "    \"save_steps\": 500,\n",
    "    \"logging_steps\": 100,\n",
    "    \"report_to\": \"tensorboard\",\n",
    "    \n",
    "    # Generation settings for evaluation\n",
    "    \"generation_max_length\": 225,\n",
    "    \"suppress_tokens\": [-1],  # Suppress special tokens\n",
    "    \n",
    "    # Low-resource optimizations\n",
    "    \"freeze_encoder\": False,  # Set to True if very limited data\n",
    "    \"freeze_feature_extractor\": True,\n",
    "    \"dropout\": 0.1,\n",
    "}\n",
    "\n",
    "print(\"üìã Configuration loaded for African language fine-tuning:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Phase 1: Data Collection & Preparation\n",
    "\n",
    "This implements **Phase 1** from your methodology - preparing the seed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare the dataset\n",
    "def load_and_prepare_dataset():\n",
    "    \"\"\"\n",
    "    Load dataset from various sources and prepare for training.\n",
    "    Supports multiple formats commonly used for African language datasets.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Option 1: Load from Hugging Face Hub\n",
    "    try:\n",
    "        print(f\"üì• Loading dataset: {CONFIG['dataset_name']}\")\n",
    "        dataset = load_dataset(CONFIG[\"dataset_name\"])\n",
    "        print(f\"‚úÖ Loaded dataset from Hugging Face Hub\")\n",
    "        return dataset\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to load from HF Hub: {e}\")\n",
    "    \n",
    "    # Option 2: Load Common Voice (if available for your language)\n",
    "    try:\n",
    "        print(f\"üì• Attempting to load Common Voice for {CONFIG['language']}...\")\n",
    "        dataset = load_dataset(\"mozilla-foundation/common_voice_11_0\", CONFIG[\"language\"])\n",
    "        print(f\"‚úÖ Loaded Common Voice dataset\")\n",
    "        return dataset\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Common Voice not available for {CONFIG['language']}: {e}\")\n",
    "    \n",
    "    # Option 3: Create from local files\n",
    "    print(\"üìÅ Creating dataset from local files...\")\n",
    "    return create_dataset_from_local_files()\n",
    "\n",
    "def create_dataset_from_local_files():\n",
    "    \"\"\"\n",
    "    Create dataset from local audio files and transcriptions.\n",
    "    Adapt this function based on your data structure.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Example structure - modify based on your data\n",
    "    audio_dir = \"./audio_files\"  # Directory with .wav files\n",
    "    transcript_file = \"./transcripts.csv\"  # CSV with filename,transcription\n",
    "    \n",
    "    if not os.path.exists(audio_dir) or not os.path.exists(transcript_file):\n",
    "        print(\"‚ùå Local files not found. Please prepare your data or use a different source.\")\n",
    "        print(\"Expected structure:\")\n",
    "        print(\"  ./audio_files/audio001.wav, audio002.wav, ...\")\n",
    "        print(\"  ./transcripts.csv with columns: filename,transcription\")\n",
    "        return None\n",
    "    \n",
    "    # Load transcriptions\n",
    "    df = pd.read_csv(transcript_file)\n",
    "    \n",
    "    # Prepare data for Dataset creation\n",
    "    data = []\n",
    "    for _, row in df.iterrows():\n",
    "        audio_path = os.path.join(audio_dir, row['filename'])\n",
    "        if os.path.exists(audio_path):\n",
    "            data.append({\n",
    "                \"audio\": audio_path,\n",
    "                \"sentence\": row['transcription']\n",
    "            })\n",
    "    \n",
    "    # Create Dataset\n",
    "    dataset = Dataset.from_list(data)\n",
    "    dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=CONFIG[\"sampling_rate\"]))\n",
    "    \n",
    "    return DatasetDict({\"train\": dataset})\n",
    "\n",
    "# Load the dataset\n",
    "raw_dataset = load_and_prepare_dataset()\n",
    "\n",
    "if raw_dataset:\n",
    "    print(f\"‚úÖ Dataset loaded successfully!\")\n",
    "    print(f\"üìä Dataset info:\")\n",
    "    for split_name, split_data in raw_dataset.items():\n",
    "        print(f\"  {split_name}: {len(split_data)} samples\")\n",
    "        print(f\"  Columns: {split_data.column_names}\")\n",
    "else:\n",
    "    print(\"‚ùå Failed to load dataset. Please check your data sources.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing and cleaning\n",
    "def preprocess_dataset(dataset):\n",
    "    \"\"\"\n",
    "    Clean and preprocess the dataset for optimal training.\n",
    "    \"\"\"\n",
    "    \n",
    "    def normalize_text(text):\n",
    "        \"\"\"\n",
    "        Normalize text for better training consistency.\n",
    "        Adapt normalization rules for your target African language.\n",
    "        \"\"\"\n",
    "        # Basic cleaning\n",
    "        text = text.strip().lower()\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "        # Language-specific normalizations\n",
    "        if CONFIG[\"language\"] == \"sw\":  # Swahili-specific\n",
    "            # Handle common Swahili contractions or variations\n",
    "            text = text.replace(\"si\", \"sio\")\n",
    "        elif CONFIG[\"language\"] == \"ha\":  # Hausa-specific\n",
    "            # Add Hausa-specific normalizations\n",
    "            pass\n",
    "        elif CONFIG[\"language\"] == \"yo\":  # Yoruba-specific\n",
    "            # Add Yoruba-specific normalizations\n",
    "            pass\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def filter_audio_length(example):\n",
    "        \"\"\"\n",
    "        Filter audio based on length criteria.\n",
    "        \"\"\"\n",
    "        duration = len(example[\"audio\"][\"array\"]) / example[\"audio\"][\"sampling_rate\"]\n",
    "        return (CONFIG[\"min_audio_length\"] <= duration <= CONFIG[\"max_audio_length\"])\n",
    "    \n",
    "    def prepare_example(example):\n",
    "        \"\"\"\n",
    "        Prepare individual examples for training.\n",
    "        \"\"\"\n",
    "        # Normalize transcription\n",
    "        example[\"sentence\"] = normalize_text(example[\"sentence\"])\n",
    "        \n",
    "        # Ensure audio is at correct sampling rate\n",
    "        if example[\"audio\"][\"sampling_rate\"] != CONFIG[\"sampling_rate\"]:\n",
    "            # Resample if needed\n",
    "            audio_array = librosa.resample(\n",
    "                example[\"audio\"][\"array\"], \n",
    "                orig_sr=example[\"audio\"][\"sampling_rate\"],\n",
    "                target_sr=CONFIG[\"sampling_rate\"]\n",
    "            )\n",
    "            example[\"audio\"] = {\n",
    "                \"array\": audio_array,\n",
    "                \"sampling_rate\": CONFIG[\"sampling_rate\"]\n",
    "            }\n",
    "        \n",
    "        return example\n",
    "    \n",
    "    print(\"üîÑ Preprocessing dataset...\")\n",
    "    \n",
    "    # Apply preprocessing\n",
    "    processed_dataset = {}\n",
    "    for split_name, split_data in dataset.items():\n",
    "        print(f\"  Processing {split_name} split...\")\n",
    "        \n",
    "        # Filter by audio length\n",
    "        filtered = split_data.filter(filter_audio_length)\n",
    "        print(f\"    Filtered: {len(split_data)} ‚Üí {len(filtered)} samples\")\n",
    "        \n",
    "        # Prepare examples\n",
    "        processed = filtered.map(prepare_example, desc=\"Preparing examples\")\n",
    "        processed_dataset[split_name] = processed\n",
    "    \n",
    "    return DatasetDict(processed_dataset)\n",
    "\n",
    "# Preprocess the dataset\n",
    "if raw_dataset:\n",
    "    processed_dataset = preprocess_dataset(raw_dataset)\n",
    "    print(\"‚úÖ Dataset preprocessing completed!\")\n",
    "    \n",
    "    # Show sample\n",
    "    sample = processed_dataset[list(processed_dataset.keys())[0]][0]\n",
    "    print(f\"\\nüìù Sample example:\")\n",
    "    print(f\"  Audio shape: {sample['audio']['array'].shape}\")\n",
    "    print(f\"  Sampling rate: {sample['audio']['sampling_rate']}\")\n",
    "    print(f\"  Transcription: '{sample['sentence']}'\")\n",
    "else:\n",
    "    print(\"‚ùå Cannot preprocess dataset - no data loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Phase 3: Model Loading and Preparation\n",
    "\n",
    "Loading the pretrained Whisper model and preparing for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Whisper model components\n",
    "def load_whisper_model():\n",
    "    \"\"\"\n",
    "    Load Whisper model, tokenizer, and feature extractor.\n",
    "    \"\"\"\n",
    "    print(f\"üì• Loading Whisper model: {CONFIG['model_name_or_path']}\")\n",
    "    \n",
    "    # Load feature extractor\n",
    "    feature_extractor = WhisperFeatureExtractor.from_pretrained(CONFIG[\"model_name_or_path\"])\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = WhisperTokenizer.from_pretrained(\n",
    "        CONFIG[\"model_name_or_path\"], \n",
    "        language=CONFIG[\"language\"], \n",
    "        task=CONFIG[\"task\"]\n",
    "    )\n",
    "    \n",
    "    # Load model\n",
    "    model = WhisperForConditionalGeneration.from_pretrained(CONFIG[\"model_name_or_path\"])\n",
    "    \n",
    "    # Configure model for fine-tuning\n",
    "    model.config.forced_decoder_ids = None\n",
    "    model.config.suppress_tokens = []\n",
    "    model.config.use_cache = False  # Required for gradient checkpointing\n",
    "    \n",
    "    # Language-specific configuration\n",
    "    if CONFIG[\"language\"] in tokenizer.get_vocab():\n",
    "        model.config.forced_decoder_ids = tokenizer.get_decoder_prompt_ids(\n",
    "            language=CONFIG[\"language\"], \n",
    "            task=CONFIG[\"task\"]\n",
    "        )\n",
    "    \n",
    "    # Freezing options for low-resource scenarios\n",
    "    if CONFIG[\"freeze_feature_extractor\"]:\n",
    "        model.freeze_feature_encoder()\n",
    "        print(\"üîí Feature extractor frozen\")\n",
    "    \n",
    "    if CONFIG[\"freeze_encoder\"]:\n",
    "        for param in model.model.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        print(\"üîí Encoder frozen\")\n",
    "    \n",
    "    # Create processor\n",
    "    processor = WhisperProcessor.from_pretrained(\n",
    "        CONFIG[\"model_name_or_path\"],\n",
    "        language=CONFIG[\"language\"],\n",
    "        task=CONFIG[\"task\"]\n",
    "    )\n",
    "    \n",
    "    return model, processor, feature_extractor, tokenizer\n",
    "\n",
    "# Load model components\n",
    "model, processor, feature_extractor, tokenizer = load_whisper_model()\n",
    "\n",
    "print(\"‚úÖ Whisper model loaded successfully!\")\n",
    "print(f\"üèóÔ∏è  Model: {model.config.name_or_path}\")\n",
    "print(f\"üåç Language: {CONFIG['language']} ({CONFIG['language_name']})\")\n",
    "print(f\"üìã Task: {CONFIG['task']}\")\n",
    "\n",
    "# Model size info\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"üìä Total parameters: {total_params:,}\")\n",
    "print(f\"üéØ Trainable parameters: {trainable_params:,} ({100*trainable_params/total_params:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Data Collation and Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom data collator for Whisper fine-tuning\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received and prepare decoder input ids.\n",
    "    \"\"\"\n",
    "    processor: Any\n",
    "    decoder_start_token_id: int\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # Split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        model_input_name = self.processor.model_input_names[0]\n",
    "        input_features = [{model_input_name: feature[model_input_name]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # Replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # If bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "\n",
    "# Prepare dataset for training\n",
    "def prepare_dataset_for_training(dataset):\n",
    "    \"\"\"\n",
    "    Apply final processing to prepare dataset for training.\n",
    "    \"\"\"\n",
    "    \n",
    "    def prepare_dataset_batch(batch):\n",
    "        # Load and process audio\n",
    "        audio = batch[\"audio\"]\n",
    "        \n",
    "        # Compute input features\n",
    "        batch[\"input_features\"] = feature_extractor(\n",
    "            audio[\"array\"], \n",
    "            sampling_rate=audio[\"sampling_rate\"]\n",
    "        ).input_features[0]\n",
    "        \n",
    "        # Encode target text\n",
    "        batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
    "        \n",
    "        return batch\n",
    "    \n",
    "    print(\"üîÑ Preparing dataset for training...\")\n",
    "    \n",
    "    prepared_dataset = {}\n",
    "    for split_name, split_data in dataset.items():\n",
    "        prepared = split_data.map(\n",
    "            prepare_dataset_batch,\n",
    "            remove_columns=split_data.column_names,\n",
    "            desc=f\"Preparing {split_name} split\"\n",
    "        )\n",
    "        prepared_dataset[split_name] = prepared\n",
    "    \n",
    "    return DatasetDict(prepared_dataset)\n",
    "\n",
    "# Create data collator\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
    "    processor=processor,\n",
    "    decoder_start_token_id=model.config.decoder_start_token_id,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Data collator created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Evaluation Metrics Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def compute_metrics(eval_preds):\n",
    "    \"\"\"\n",
    "    Compute Word Error Rate (WER) and other metrics for evaluation.\n",
    "    \"\"\"\n",
    "    pred_ids = eval_preds.predictions\n",
    "    label_ids = eval_preds.label_ids\n",
    "\n",
    "    # Replace -100 with pad token id\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    # Decode predictions and labels\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    # Compute WER\n",
    "    wer_score = jiwer.wer(label_str, pred_str)\n",
    "    \n",
    "    # Compute CER (Character Error Rate)\n",
    "    cer_score = jiwer.cer(label_str, pred_str)\n",
    "    \n",
    "    return {\n",
    "        \"wer\": wer_score,\n",
    "        \"cer\": cer_score,\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Evaluation metrics configured!\")\n",
    "print(\"üìä Metrics: WER (Word Error Rate), CER (Character Error Rate)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèÉ‚Äç‚ôÇÔ∏è Training Configuration and Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare final dataset and splits\n",
    "if processed_dataset and 'train' in processed_dataset:\n",
    "    # Prepare dataset for training\n",
    "    final_dataset = prepare_dataset_for_training(processed_dataset)\n",
    "    \n",
    "    # Create train/test split if needed\n",
    "    if 'test' not in final_dataset and 'validation' not in final_dataset:\n",
    "        print(f\"üîÑ Creating train/test split ({1-CONFIG['test_size']:.1f}/{CONFIG['test_size']:.1f})...\")\n",
    "        train_test = final_dataset['train'].train_test_split(\n",
    "            test_size=CONFIG['test_size'],\n",
    "            seed=42\n",
    "        )\n",
    "        final_dataset = DatasetDict({\n",
    "            'train': train_test['train'],\n",
    "            'test': train_test['test']\n",
    "        })\n",
    "    \n",
    "    print(\"üìä Final dataset splits:\")\n",
    "    for split_name, split_data in final_dataset.items():\n",
    "        print(f\"  {split_name}: {len(split_data)} samples\")\n",
    "        \n",
    "    train_dataset = final_dataset['train']\n",
    "    eval_dataset = final_dataset['test'] if 'test' in final_dataset else final_dataset.get('validation')\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No training data available. Please check your dataset loading.\")\n",
    "    train_dataset = None\n",
    "    eval_dataset = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "if train_dataset:\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=CONFIG[\"output_dir\"],\n",
    "        per_device_train_batch_size=CONFIG[\"per_device_train_batch_size\"],\n",
    "        per_device_eval_batch_size=CONFIG[\"per_device_eval_batch_size\"],\n",
    "        gradient_accumulation_steps=CONFIG[\"gradient_accumulation_steps\"],\n",
    "        learning_rate=CONFIG[\"learning_rate\"],\n",
    "        warmup_steps=CONFIG[\"warmup_steps\"],\n",
    "        max_steps=CONFIG[\"max_steps\"],\n",
    "        gradient_checkpointing=CONFIG[\"gradient_checkpointing\"],\n",
    "        fp16=CONFIG[\"fp16\"],\n",
    "        evaluation_strategy=CONFIG[\"evaluation_strategy\"],\n",
    "        eval_steps=CONFIG[\"eval_steps\"],\n",
    "        save_steps=CONFIG[\"save_steps\"],\n",
    "        logging_steps=CONFIG[\"logging_steps\"],\n",
    "        report_to=CONFIG[\"report_to\"],\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"wer\",\n",
    "        greater_is_better=False,\n",
    "        push_to_hub=False,  # Set to True if you want to push to HF Hub\n",
    "        remove_unused_columns=False,  # Important for Whisper\n",
    "        label_names=[\"labels\"],  # Important for Seq2Seq\n",
    "        predict_with_generate=True,\n",
    "        generation_max_length=CONFIG[\"generation_max_length\"],\n",
    "        generation_num_beams=1,  # Use beam search for better quality\n",
    "    )\n",
    "\n",
    "    print(\"üìã Training configuration:\")\n",
    "    print(f\"  Output directory: {training_args.output_dir}\")\n",
    "    print(f\"  Batch size per device: {training_args.per_device_train_batch_size}\")\n",
    "    print(f\"  Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
    "    print(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "    print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "    print(f\"  Max steps: {training_args.max_steps}\")\n",
    "    print(f\"  Mixed precision: {training_args.fp16}\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå Cannot create training configuration - no training data available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "if train_dataset:\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        args=training_args,\n",
    "        model=model,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        tokenizer=processor.feature_extractor,\n",
    "    )\n",
    "\n",
    "    print(\"‚úÖ Trainer initialized successfully!\")\n",
    "    print(f\"üìä Training samples: {len(train_dataset)}\")\n",
    "    if eval_dataset:\n",
    "        print(f\"üìä Evaluation samples: {len(eval_dataset)}\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå Cannot initialize trainer - no training data available.\")\n",
    "    trainer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "if trainer:\n",
    "    print(\"üöÄ Starting Whisper fine-tuning...\")\n",
    "    print(f\"üåç Target language: {CONFIG['language_name']} ({CONFIG['language']})\")\n",
    "    print(f\"‚è∞ Started at: {pd.Timestamp.now()}\")\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üî• TRAINING IN PROGRESS\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    try:\n",
    "        # Train the model\n",
    "        trainer.train()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"‚úÖ TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"‚è∞ Finished at: {pd.Timestamp.now()}\")\n",
    "        \n",
    "        # Save final model\n",
    "        trainer.save_model()\n",
    "        processor.save_pretrained(CONFIG[\"output_dir\"])\n",
    "        \n",
    "        print(f\"üíæ Model saved to: {CONFIG['output_dir']}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Training failed: {e}\")\n",
    "        print(\"üí° Try reducing batch_size or max_steps if you're running out of memory\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå Cannot start training - trainer not initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Phase 4: Evaluation and Testing\n",
    "\n",
    "Implementing **Phase 4** of your methodology - measuring performance improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive evaluation\n",
    "if trainer and eval_dataset:\n",
    "    print(\"üìä Running comprehensive evaluation...\")\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    eval_results = trainer.evaluate()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üìà EVALUATION RESULTS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for key, value in eval_results.items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"  {key}: {value:.4f}\")\n",
    "        else:\n",
    "            print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # WER interpretation\n",
    "    wer = eval_results.get('eval_wer', None)\n",
    "    if wer is not None:\n",
    "        print(f\"\\nüéØ Word Error Rate (WER): {wer:.2%}\")\n",
    "        if wer < 0.10:\n",
    "            print(\"üèÜ Excellent! WER < 10% - Production ready\")\n",
    "        elif wer < 0.20:\n",
    "            print(\"‚úÖ Good! WER < 20% - Very usable\")\n",
    "        elif wer < 0.30:\n",
    "            print(\"‚ö†Ô∏è  Fair! WER < 30% - Needs improvement\")\n",
    "        else:\n",
    "            print(\"‚ùå Poor! WER > 30% - Requires more training data or different approach\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå Cannot run evaluation - no trained model or evaluation data available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Interactive Testing and Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the fine-tuned model on sample audio\n",
    "def test_model_inference(audio_file_path=None):\n",
    "    \"\"\"\n",
    "    Test the fine-tuned model on a sample audio file.\n",
    "    \"\"\"\n",
    "    if not trainer:\n",
    "        print(\"‚ùå No trained model available for testing\")\n",
    "        return\n",
    "    \n",
    "    # Use sample from dataset if no file provided\n",
    "    if audio_file_path is None and eval_dataset:\n",
    "        print(\"üéß Testing on sample from evaluation dataset...\")\n",
    "        sample = eval_dataset[0]\n",
    "        \n",
    "        # Reconstruct audio from features (approximate)\n",
    "        # For actual testing, use original audio file\n",
    "        print(\"üìù Ground truth transcription:\")\n",
    "        ground_truth = tokenizer.decode(sample['labels'], skip_special_tokens=True)\n",
    "        print(f\"  '{ground_truth}'\")\n",
    "        \n",
    "        # Generate prediction\n",
    "        input_features = torch.tensor([sample['input_features']]).to(model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            predicted_ids = model.generate(\n",
    "                input_features,\n",
    "                max_length=CONFIG[\"generation_max_length\"],\n",
    "                num_beams=1,\n",
    "                do_sample=False\n",
    "            )\n",
    "        \n",
    "        prediction = tokenizer.decode(predicted_ids[0], skip_special_tokens=True)\n",
    "        \n",
    "        print(\"\\nü§ñ Model prediction:\")\n",
    "        print(f\"  '{prediction}'\")\n",
    "        \n",
    "        # Calculate WER for this sample\n",
    "        sample_wer = jiwer.wer(ground_truth, prediction)\n",
    "        print(f\"\\nüìä Sample WER: {sample_wer:.2%}\")\n",
    "    \n",
    "    elif audio_file_path and os.path.exists(audio_file_path):\n",
    "        print(f\"üéß Testing on audio file: {audio_file_path}\")\n",
    "        \n",
    "        # Load and process audio\n",
    "        audio, sr = librosa.load(audio_file_path, sr=CONFIG[\"sampling_rate\"])\n",
    "        input_features = feature_extractor(audio, sampling_rate=sr, return_tensors=\"pt\")\n",
    "        \n",
    "        # Generate transcription\n",
    "        with torch.no_grad():\n",
    "            predicted_ids = model.generate(\n",
    "                input_features.input_features.to(model.device),\n",
    "                max_length=CONFIG[\"generation_max_length\"],\n",
    "                num_beams=2,\n",
    "                do_sample=False\n",
    "            )\n",
    "        \n",
    "        prediction = tokenizer.decode(predicted_ids[0], skip_special_tokens=True)\n",
    "        \n",
    "        print(f\"\\nü§ñ Transcription: '{prediction}'\")\n",
    "        print(f\"‚è±Ô∏è  Audio duration: {len(audio)/sr:.1f} seconds\")\n",
    "    \n",
    "    else:\n",
    "        print(\"‚ùå No audio file provided or evaluation dataset unavailable\")\n",
    "        print(\"üí° Provide an audio file path to test: test_model_inference('path/to/audio.wav')\")\n",
    "\n",
    "# Test the model\n",
    "test_model_inference()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Training Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive training summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä WHISPER FINE-TUNING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nüéØ Target Language: {CONFIG['language_name']} ({CONFIG['language']})\")\n",
    "print(f\"ü§ñ Base Model: {CONFIG['model_name_or_path']}\")\n",
    "print(f\"üìã Task: {CONFIG['task']}\")\n",
    "\n",
    "if trainer:\n",
    "    print(f\"\\nüìä Training Configuration:\")\n",
    "    print(f\"  Training samples: {len(train_dataset) if train_dataset else 0:,}\")\n",
    "    print(f\"  Evaluation samples: {len(eval_dataset) if eval_dataset else 0:,}\")\n",
    "    print(f\"  Batch size: {CONFIG['per_device_train_batch_size']}\")\n",
    "    print(f\"  Learning rate: {CONFIG['learning_rate']}\")\n",
    "    print(f\"  Max steps: {CONFIG['max_steps']}\")\n",
    "    \n",
    "    if 'eval_results' in locals():\n",
    "        print(f\"\\nüìà Final Results:\")\n",
    "        wer = eval_results.get('eval_wer')\n",
    "        cer = eval_results.get('eval_cer')\n",
    "        if wer: print(f\"  Word Error Rate (WER): {wer:.2%}\")\n",
    "        if cer: print(f\"  Character Error Rate (CER): {cer:.2%}\")\n",
    "    \n",
    "    print(f\"\\nüíæ Model Output:\")\n",
    "    print(f\"  Saved to: {CONFIG['output_dir']}\")\n",
    "    print(f\"  Files: pytorch_model.bin, config.json, preprocessor_config.json\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n‚ùå Training was not completed successfully\")\n",
    "\n",
    "print(f\"\\nüîÑ Phase 4 Next Steps:\")\n",
    "print(\"  1. ‚úÖ Evaluate WER performance (completed)\")\n",
    "print(\"  2. üîÑ Test on diverse audio samples\")\n",
    "print(\"  3. üîÑ Compare with baseline Whisper performance\")\n",
    "print(\"  4. üîÑ Iterate: Add more synthetic data if WER > 20%\")\n",
    "print(\"  5. üîÑ Deploy for production use if WER < 10%\")\n",
    "\n",
    "print(f\"\\nüöÄ Deployment Options:\")\n",
    "print(\"  - Save to Hugging Face Hub for easy sharing\")\n",
    "print(\"  - Convert to ONNX for faster inference\")\n",
    "print(\"  - Quantize for mobile deployment\")\n",
    "print(\"  - Integrate into speech-to-speech pipeline\")\n",
    "\n",
    "print(f\"\\nüìö Resources for African Language STT:\")\n",
    "print(\"  - Mozilla Common Voice: https://commonvoice.mozilla.org/\")\n",
    "print(\"  - OpenSLR African Languages: http://openslr.org/\")\n",
    "print(\"  - Masakhane Community: https://www.masakhane.io/\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ WHISPER FINE-TUNING PIPELINE COMPLETE!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Advanced: Integration with Phase 2 Synthetic Data\n",
    "\n",
    "This section shows how to integrate synthetic data generated from Phase 2 of your methodology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example integration with synthetic data (Phase 2)\n",
    "def integrate_synthetic_data(original_dataset, synthetic_audio_dir, synthetic_transcripts_file):\n",
    "    \"\"\"\n",
    "    Combine real dataset with synthetic data generated from voice cloning platforms.\n",
    "    \n",
    "    This implements the combination strategy from Phase 2 of your methodology.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not os.path.exists(synthetic_audio_dir) or not os.path.exists(synthetic_transcripts_file):\n",
    "        print(\"‚ÑπÔ∏è  Synthetic data not found - using only real data\")\n",
    "        print(\"üí° Generate synthetic data using Phase 2 pipeline first:\")\n",
    "        print(\"   - Use ElevenLabs, Podcastle, or Cartesia for voice cloning\")\n",
    "        print(\"   - Generate thousands of hours of synthetic speech\")\n",
    "        print(\"   - Save audio files and transcriptions\")\n",
    "        return original_dataset\n",
    "    \n",
    "    print(\"üîÑ Integrating synthetic data with real dataset...\")\n",
    "    \n",
    "    # Load synthetic transcripts\n",
    "    synthetic_df = pd.read_csv(synthetic_transcripts_file)\n",
    "    \n",
    "    # Prepare synthetic data\n",
    "    synthetic_data = []\n",
    "    for _, row in synthetic_df.iterrows():\n",
    "        audio_path = os.path.join(synthetic_audio_dir, row['filename'])\n",
    "        if os.path.exists(audio_path):\n",
    "            synthetic_data.append({\n",
    "                \"audio\": audio_path,\n",
    "                \"sentence\": row['transcription'],\n",
    "                \"is_synthetic\": True  # Flag to track synthetic samples\n",
    "            })\n",
    "    \n",
    "    # Create synthetic dataset\n",
    "    synthetic_dataset = Dataset.from_list(synthetic_data)\n",
    "    synthetic_dataset = synthetic_dataset.cast_column(\"audio\", Audio(sampling_rate=CONFIG[\"sampling_rate\"]))\n",
    "    \n",
    "    # Add synthetic flag to original data\n",
    "    original_with_flag = original_dataset['train'].map(lambda x: {**x, \"is_synthetic\": False})\n",
    "    \n",
    "    # Combine datasets\n",
    "    from datasets import concatenate_datasets\n",
    "    combined_dataset = concatenate_datasets([original_with_flag, synthetic_dataset])\n",
    "    \n",
    "    # Shuffle the combined dataset\n",
    "    combined_dataset = combined_dataset.shuffle(seed=42)\n",
    "    \n",
    "    print(f\"‚úÖ Combined dataset created:\")\n",
    "    print(f\"  Original samples: {len(original_with_flag)}\")\n",
    "    print(f\"  Synthetic samples: {len(synthetic_dataset)}\")\n",
    "    print(f\"  Total samples: {len(combined_dataset)}\")\n",
    "    print(f\"  Synthetic ratio: {len(synthetic_dataset)/len(combined_dataset):.1%}\")\n",
    "    \n",
    "    return DatasetDict({\"train\": combined_dataset})\n",
    "\n",
    "# Example usage (uncomment when you have synthetic data)\n",
    "# combined_dataset = integrate_synthetic_data(\n",
    "#     processed_dataset,\n",
    "#     \"./synthetic_audio/\",  # Directory with synthetic audio files\n",
    "#     \"./synthetic_transcripts.csv\"  # CSV with synthetic transcriptions\n",
    "# )\n",
    "\n",
    "print(\"üí° Synthetic data integration ready!\")\n",
    "print(\"   Run the integration function when you have synthetic data from Phase 2\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}